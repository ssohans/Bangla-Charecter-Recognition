{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"step(11).ipynb","provenance":[],"mount_file_id":"16Y2IdbwprAKmJA9591SBZ8-8DO58897J","authorship_tag":"ABX9TyN726cEf5WCov/KA7uNnCNb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DU1F2fZmmR4s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"6cb1593e-b293-4f08-b4d6-d746462c5bc6","executionInfo":{"status":"ok","timestamp":1583509036083,"user_tz":-360,"elapsed":3505,"user":{"displayName":"Sayed Sohan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2_u1vq2AAyvj3aMvkKehjd9lOAfPdEHOkjt-hPQ=s64","userId":"05210628742846194503"}}},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from tqdm.auto import tqdm\n","from glob import glob\n","import time, gc\n","import cv2\n","import zipfile\n","import tensorflow as tf\n","from keras.utils.np_utils import to_categorical\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from tensorflow import keras\n","import matplotlib.image as mpimg\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Model\n","from keras.models import clone_model\n","from keras.layers import *\n","from keras.optimizers import Adam\n","from keras.callbacks import ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from keras.optimizers import RMSprop\n","from keras.applications import VGG16\n","from keras.applications import VGG19\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output.\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8UfDr77fotYX","colab_type":"code","colab":{}},"source":["# !pip uninstall tensorflow\n","# !pip install tensorflow\n","# !pip uninstall keras\n","# !pip install keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jt6MXPhvp6Mv","colab_type":"code","colab":{}},"source":["inputs = Input(shape=(64,64,1))\n","model = Conv2D(32, (3,3), padding='same')(inputs)\n","model = BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\")(model)\n","model = LeakyReLU(alpha=0.1)(model)\n","model = Conv2D(32,  (3,3), padding='same')(model)\n","model = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\")(model)\n","model = MaxPooling2D(2, 2)(model)\n","model = Dropout(0.2)(model)\n","model = Conv2D(64, (3,3), padding='same')(model)\n","model = BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=\"uniform\")(model)\n","model = LeakyReLU(alpha=0.1)(model)\n","model = Conv2D(64, (3,3), padding='same')(model)\n","model = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\")(model)\n","model = LeakyReLU(alpha=0.1)(model)\n","model = MaxPooling2D(2,2)(model)\n","model = Dropout(0.2)(model)\n","model = Conv2D(128, (3,3), padding='same')(model)\n","model = BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=\"uniform\")(model)\n","model = LeakyReLU(alpha=0.1)(model)\n","model = Conv2D(128, (3,3), padding='same')(model)\n","model = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\")(model)\n","model = LeakyReLU(alpha=0.1)(model)\n","model = MaxPooling2D(2,2)(model)\n","model = Dropout(0.2)(model)\n","model = Flatten()(model)\n","model = Dense(512)(model)\n","model = LeakyReLU(alpha=0.1)(model)\n","model = BatchNormalization()(model)\n","head = Dense(168, activation='softmax',name = 'head')(model)\n","vowel = Dense(11, activation='softmax',name = 'vowel')(model)\n","consonant = Dense(7, activation='softmax',name = 'consonant')(model)\n","\n","model = Model(inputs=inputs, outputs=[head,vowel,consonant])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qd3YaDfs1mt","colab_type":"code","colab":{}},"source":["# from keras.utils import plot_model\n","# plot_model(model, to_file='model.png')\n","# model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"drCUK6nGtkDT","colab_type":"code","colab":{}},"source":["from keras.callbacks import EarlyStopping\n","initial_learningrate=2e-3\n","batch_size = 256\n","epochs = 30\n","es = EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n","model.compile(loss=\"categorical_crossentropy\",\n","              optimizer=RMSprop(lr=initial_learningrate),\n","              metrics=['accuracy'])\n","\n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n","                                            patience=10,\n","                                            verbose=1,\n","                                            factor=0.75)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyx_1FhKuVBj","colab_type":"code","colab":{}},"source":["def train_img(namelist):\n","    x = []\n","    with zipfile.ZipFile('/content/drive/My Drive/Colab Notebooks/Datasets/Bangla AI/train2.zip', 'r') as zfile:\n","        for i in namelist:\n","            data = cv2.imdecode(np.frombuffer(zfile.read(i), np.uint8), 0).astype('float32')\n","            data = data/255\n","            x.append(data)\n","    \n","    return x\n","\n","def test_img():\n","    x = []\n","    with zipfile.ZipFile('/content/drive/My Drive/Colab Notebooks/Datasets/Bangla AI/test.zip', 'r') as zfile:\n","        name = zfile.namelist()\n","        for i in name:\n","            data = cv2.imdecode(np.frombuffer(zfile.read(i), np.uint8), 0).astype('float32')\n","            data = data/255\n","            x.append(data)\n","    \n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtG7_vcKuddp","colab_type":"code","colab":{}},"source":["df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/Bangla AI/train.csv\")\n","df.head()\n","ls = df['image_id'].tolist()\n","ls = [i+'.png' for i in ls ]\n","# x = ls[:40000]\n","# xx = ls[len(x):80000]\n","# xxx = ls[80000:120000]\n","# xxxx= ls[120000:160000]\n","# xxxxx = ls[160000:]\n","z = df.iloc[:,1:4].values.tolist()\n","# y = z[:40000]\n","# yy = z[40000:80000]\n","# yyy = z[80000:120000]\n","# yyyy = z[120000:160000]\n","# yyyyy = z[160000:]\n","\n","label = z\n","name = ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZubuIS7ufQZ","colab_type":"code","colab":{}},"source":["class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n","\n","    def flow(self,\n","             x,\n","             y=None,\n","             batch_size=32,\n","             shuffle=True,\n","             sample_weight=None,\n","             seed=None,\n","             save_to_dir=None,\n","             save_prefix='',\n","             save_format='png',\n","             subset=None):\n","\n","        targets = None\n","        target_lengths = {}\n","        ordered_outputs = []\n","        for output, target in y.items():\n","            if targets is None:\n","                targets = target\n","            else:\n","                targets = np.concatenate((targets, target), axis=1)\n","            target_lengths[output] = target.shape[1]\n","            ordered_outputs.append(output)\n","\n","\n","        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n","                                         shuffle=shuffle):\n","            target_dict = {}\n","            i = 0\n","            for output in ordered_outputs:\n","                target_length = target_lengths[output]\n","                target_dict[output] = flowy[:, i: i + target_length]\n","                i += target_length\n","\n","            yield flowx, target_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymPDpWJSugsz","colab_type":"code","colab":{}},"source":["datagen = MultiOutputDataGenerator(\n","    featurewise_center=False,  # set input mean to 0 over the dataset\n","    samplewise_center=False,  # set each sample mean to 0\n","    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","    samplewise_std_normalization=False,  # divide each input by its std\n","    zca_whitening=False,  # apply ZCA whitening\n","    rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n","    zoom_range = 0.15, # Randomly zoom image \n","    width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n","    height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n","    horizontal_flip=False,  # randomly flip images\n","    vertical_flip=False)  # randomly flip images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZ8Igbt0u8JQ","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from keras.models import load_model\n","\n","model = load_model('/content/drive/My Drive/Colab Notebooks/kan4model.h5')\n","train_image = np.array(train_img(name))\n","train_image = train_image.reshape(train_image.shape[0], train_image.shape[1], train_image.shape[2], 1)\n","label = np.array(label)\n","head = np.array(label[:,0].tolist())\n","vowel = np.array(label[:,1].tolist())\n","consonant = np.array(label[:,2].tolist())\n","head = to_categorical(head, num_classes = 168)\n","vowel = to_categorical(vowel,num_classes=11)\n","consonant = to_categorical(consonant,num_classes=7)\n","\n","x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(train_image, head, vowel, \n","                                                                                                                                consonant, test_size=0.08, random_state=666)\n","datagen.fit(x_train)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ams70tYj9se0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":304},"outputId":"4db56792-6dfd-4f43-a8d4-2ba36bed985a"},"source":["\n","history = model.fit_generator(datagen.flow(x_train, {'head': y_train_root, 'vowel': y_train_vowel, 'consonant': y_train_consonant}, batch_size=batch_size),\n","                          epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n","                          steps_per_epoch=x_train.shape[0] // batch_size, \n","                          callbacks=[learning_rate_reduction,es])\n","model.save('/content/drive/My Drive/Colab Notebooks/kan4model.h5')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","721/721 [==============================] - 237s 329ms/step - loss: 0.3657 - head_loss: 0.2014 - vowel_loss: 0.0854 - consonant_loss: 0.0788 - head_accuracy: 0.9393 - vowel_accuracy: 0.9744 - consonant_accuracy: 0.9752 - val_loss: 0.3844 - val_head_loss: 0.2331 - val_vowel_loss: 0.0790 - val_consonant_loss: 0.0727 - val_head_accuracy: 0.9416 - val_vowel_accuracy: 0.9816 - val_consonant_accuracy: 0.9806\n","Epoch 2/30\n","721/721 [==============================] - 237s 329ms/step - loss: 0.3618 - head_loss: 0.1991 - vowel_loss: 0.0840 - consonant_loss: 0.0787 - head_accuracy: 0.9394 - vowel_accuracy: 0.9749 - consonant_accuracy: 0.9756 - val_loss: 0.3891 - val_head_loss: 0.2292 - val_vowel_loss: 0.0851 - val_consonant_loss: 0.0750 - val_head_accuracy: 0.9430 - val_vowel_accuracy: 0.9790 - val_consonant_accuracy: 0.9800\n","Epoch 3/30\n","721/721 [==============================] - 237s 329ms/step - loss: 0.3587 - head_loss: 0.1978 - vowel_loss: 0.0830 - consonant_loss: 0.0779 - head_accuracy: 0.9402 - vowel_accuracy: 0.9757 - consonant_accuracy: 0.9754 - val_loss: 0.4091 - val_head_loss: 0.2463 - val_vowel_loss: 0.0850 - val_consonant_loss: 0.0777 - val_head_accuracy: 0.9404 - val_vowel_accuracy: 0.9806 - val_consonant_accuracy: 0.9792\n","Epoch 4/30\n","721/721 [==============================] - 238s 329ms/step - loss: 0.3544 - head_loss: 0.1947 - vowel_loss: 0.0818 - consonant_loss: 0.0779 - head_accuracy: 0.9410 - vowel_accuracy: 0.9757 - consonant_accuracy: 0.9753 - val_loss: 0.3821 - val_head_loss: 0.2326 - val_vowel_loss: 0.0768 - val_consonant_loss: 0.0728 - val_head_accuracy: 0.9413 - val_vowel_accuracy: 0.9813 - val_consonant_accuracy: 0.9808\n","Epoch 5/30\n","721/721 [==============================] - 237s 329ms/step - loss: 0.3542 - head_loss: 0.1935 - vowel_loss: 0.0832 - consonant_loss: 0.0775 - head_accuracy: 0.9411 - vowel_accuracy: 0.9749 - consonant_accuracy: 0.9760 - val_loss: 0.4427 - val_head_loss: 0.2542 - val_vowel_loss: 0.1016 - val_consonant_loss: 0.0869 - val_head_accuracy: 0.9379 - val_vowel_accuracy: 0.9759 - val_consonant_accuracy: 0.9783\n","Epoch 6/30\n","721/721 [==============================] - 237s 329ms/step - loss: 0.3524 - head_loss: 0.1932 - vowel_loss: 0.0827 - consonant_loss: 0.0764 - head_accuracy: 0.9411 - vowel_accuracy: 0.9756 - consonant_accuracy: 0.9766 - val_loss: 0.3772 - val_head_loss: 0.2302 - val_vowel_loss: 0.0758 - val_consonant_loss: 0.0713 - val_head_accuracy: 0.9399 - val_vowel_accuracy: 0.9801 - val_consonant_accuracy: 0.9798\n","Epoch 7/30\n","721/721 [==============================] - 238s 329ms/step - loss: 0.3505 - head_loss: 0.1925 - vowel_loss: 0.0820 - consonant_loss: 0.0761 - head_accuracy: 0.9412 - vowel_accuracy: 0.9758 - consonant_accuracy: 0.9761 - val_loss: 0.3762 - val_head_loss: 0.2268 - val_vowel_loss: 0.0782 - val_consonant_loss: 0.0713 - val_head_accuracy: 0.9430 - val_vowel_accuracy: 0.9803 - val_consonant_accuracy: 0.9803\n","Epoch 8/30\n","686/721 [===========================>..] - ETA: 11s - loss: 0.3485 - head_loss: 0.1913 - vowel_loss: 0.0819 - consonant_loss: 0.0753 - head_accuracy: 0.9411 - vowel_accuracy: 0.9755 - consonant_accuracy: 0.9763"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cMAnqEhnVP-G","colab_type":"code","colab":{}},"source":["del train_image\n","del vowel\n","del consonant\n","del head\n","del x_train"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QIstGN6vB-a","colab_type":"code","colab":{}},"source":["x = np.array(test_img())\n","\n","x = x.reshape(x.shape[0],64,64,1)\n","\n","z = model.predict(x)\n","\n","x = np.argmax(z[0], axis=1)\n","y = np.argmax(z[1], axis=1)\n","z = np.argmax(z[2], axis=1)\n","print(x)"],"execution_count":0,"outputs":[]}]}